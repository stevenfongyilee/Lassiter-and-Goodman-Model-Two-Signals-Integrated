# Lassiter-and-Goodman-Model-Two-Signals-Integrated
This repository is part a project that comprises a collection of models of statistical inference from utterances containing vague predicates, based on work from Lassiter & Goodman (2017). They adopt a version of the threshold semantics of Kennedy (2007) 'Vagueness and Grammar', and then explain statistical inferences from utterances containing vague predicates as a pragmatic process. Their models are also known as rational speech act (RSA) models; they start with truth-conditonal semantics and adapt Gricean pragmatics to Bayesian probabilistic reasoning.

On these models, a listener tries to reason how a rational speaker would behave, if that speaker was in turn reasoning about how a literal listener who assumes the utterance is true, would interpret that utterance. Such a strategic listener attempts to determine the probability that a rational speaker would call someone 'tall', if that someone was h feet tall, and the threshold for 'x is tall' being true, was t; such a rational speaker would, assuming they desire to be maximally informative about that someone's height, choose strategically among the various words in the language to maximize the informativity of the utterance to the literal listener, while minimizing the cost to themselves of making such an utterance, where the cost is determined by the length of the utterance.

The model in this repository is an implementation of their model, for the case when the speaker is assumed to be choosing between 'tall' and saying nothing at all. In contrast to the other repositories in this project, we here use scipy's integrate module in check that the resulting values for the listener's posterior probability density functions for h and t are consistent with the values resulting from taking the normalization factor to be simply the sum of the non-normalized posterior probability densities.
